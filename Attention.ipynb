{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  \n",
    "        att2 = self.decoder_att(decoder_hidden)  \n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2) \n",
    "        alpha = self.softmax(att)  \n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) \n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.fc.out_features        \n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  \n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        \n",
    "        h, c = torch.zeros(batch_size, decoder_dim).to(device), torch.zeros(batch_size, decoder_dim).to(device)        \n",
    "        predictions = torch.zeros(batch_size, max(caption_lengths), vocab_size).to(device)\n",
    "    \n",
    "        for t in range(max(caption_lengths)):\n",
    "            batch_size_t = sum([l > t for l in caption_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            \n",
    "            # Fix: Apply embedding layer to the encoded captions indices\n",
    "            embedded = self.embedding(encoded_captions[:batch_size_t, t])\n",
    "            \n",
    "            # Now concatenate the embedded vector with attention_weighted_encoding\n",
    "            lstm_input = torch.cat([embedded, attention_weighted_encoding], dim=1)\n",
    "            \n",
    "            h, c = self.lstm(lstm_input, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "        return predictions, encoded_captions, caption_lengths, sort_ind\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_dim, decoder_dim, attention_dim, vocab_size, encoder_dim=512, dropout=0.5):\n",
    "        super(MNISTCaptioningModel, self).__init__()\n",
    "        # Use a simple CNN as the encoder for MNIST\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, encoder_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        )\n",
    "        # Fix: Changed DecoderwithAttention to DecoderWithAttention\n",
    "        self.decoder = DecoderWithAttention(attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout)\n",
    "\n",
    "    def forward(self, images, encoded_captions, caption_lengths):\n",
    "        encoder_out = self.encoder(images)  \n",
    "        encoder_out = encoder_out.view(encoder_out.size(0), -1, encoder_out.size(1))  # (batch_size, 1, encoder_dim)\n",
    "        predictions, encoded_captions, caption_lengths, sort_ind = self.decoder(encoder_out, encoded_captions, caption_lengths)\n",
    "        return predictions, encoded_captions, caption_lengths, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "vocab = [\"<start>\", \"<end>\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"This\", \"is\", \"the\", \"digit\"]\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_caption(caption):\n",
    "    return [word_to_idx[word] for word in caption.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\"This is the digit 1\", \"This is the digit 2\", \"This is the digit 3\", \"This is the digit 4\", \"This is the digit 6\", \"This is the digit 7\", \"This is the digit 8\", \"This is the digit 5\", \"This is the digit 0\", \"This is the digit 9\"]\n",
    "\n",
    "encoded_captions = [encode_caption(caption) for caption in captions]\n",
    "max_caption_length = max(len(caption) for caption in encoded_captions)\n",
    "padded_captions = [caption + [word_to_idx[\"<end>\"]] * (max_caption_length - len(caption)) for caption in encoded_captions]\n",
    "encoded_captions = torch.tensor(padded_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_dim = 256\n",
    "decoder_dim = 512\n",
    "attention_dim = 512\n",
    "model = MNISTCaptioningModel(embed_dim, decoder_dim, attention_dim, vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[\"<end>\"])  # Ignore padding index\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_dataset = datasets.MNIST(root =\\'./data\\',train = True,download = True, transform = transform)\\ntest_dataset = datasets.MNIST(root =\\'./data\\',train = False,download = True, transform = transform)\\n\\ntrain_loader = DataLoader(dataset = train_dataset, batch_size = 128, shuffle = True)\\ntest_loader = DataLoader(dataset = test_dataset, batch_size = 128, shuffle = False)\\n\\ndevice = torch.device(\"cude\" if torch.cuda.is_available() else \"cpu\")'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_dataset = datasets.MNIST(root ='./data',train = True,download = True, transform = transform)\n",
    "test_dataset = datasets.MNIST(root ='./data',train = False,download = True, transform = transform)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 128, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = 128, shuffle = False)\n",
    "\n",
    "device = torch.device(\"cude\" if torch.cuda.is_available() else \"cpu\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Use labels as captions (e.g., \"This is the digit 5\")\n",
    "        target_captions = [f\"This is the digit {label}\" for label in labels.tolist()]\n",
    "        encoded_targets = [encode_caption(caption) for caption in target_captions]\n",
    "        padded_targets = [caption + [word_to_idx[\"<end>\"]] * (max_caption_length - len(caption)) for caption in encoded_targets]\n",
    "        encoded_targets = torch.tensor(padded_targets).to(device)\n",
    "\n",
    "\n",
    "        predictions, _, _, _ = model(images, encoded_targets, torch.tensor([max_caption_length] * batch_size).to(device))\n",
    "        predictions = predictions.view(-1, vocab_size)\n",
    "        targets = encoded_targets.view(-1)\n",
    "        loss = criterion(predictions, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        encoder_out = model.encoder(image)\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_out.size(1))\n",
    "\n",
    "        \n",
    "        caption = [word_to_idx[\"<start>\"]]\n",
    "        h, c = torch.zeros(1, decoder_dim).to(device), torch.zeros(1, decoder_dim).to(device)\n",
    "\n",
    "        for _ in range(max_caption_length):\n",
    "            encoded_caption = torch.tensor(caption).unsqueeze(0).to(device)\n",
    "            predictions, _, _, _ = model.decoder(encoder_out, encoded_caption, torch.tensor([len(caption)]).to(device))\n",
    "            next_word_idx = predictions.argmax(2)[:, -1].item()\n",
    "            caption.append(next_word_idx)\n",
    "            if next_word_idx == word_to_idx[\"<end>\"]:\n",
    "                break\n",
    "\n",
    "\n",
    "        caption_words = [idx_to_word[idx] for idx in caption if idx not in [word_to_idx[\"<start>\"], word_to_idx[\"<end>\"]]]\n",
    "        return \" \".join(caption_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = dataset[0]\n",
    "predicted_caption = predict_caption(model, test_image)\n",
    "print(f\"Predicted Caption: {predicted_caption}\")\n",
    "print(f\"Actual Label: {test_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
